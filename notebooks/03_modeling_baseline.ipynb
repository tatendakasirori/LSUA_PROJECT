{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d74a18",
   "metadata": {},
   "source": [
    "# Training Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0975f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    StackingRegressor\n",
    ")\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.svm import SVR\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab7fd214",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading data\n",
    "df = pd.read_csv(\"../data/processed/qm9_mordred_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f590dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Features and Targets\n",
    "targets = [\"mu\", \"HOMO\", \"LUMO\", \"gap\"]\n",
    "X = df.drop(columns=[\"smiles\"] + targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f0d17f",
   "metadata": {},
   "source": [
    "#### Train Ridge Regression for each target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for target in targets:\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = Ridge()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    metrics.append({\n",
    "        \"target\": target,\n",
    "        \"model\": \"Ridge\",\n",
    "        \"r2_score\": r2,\n",
    "        \"mse\": mse\n",
    "    })\n",
    "\n",
    "# --- Save metrics to CSV ---\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df.to_csv(\"../results/metrics/ridge_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8bc86a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ridge Regression Performance Metrics:\n",
      "\n",
      "target model  r2_score    mse\n",
      "    mu Ridge    0.3406 1.4616\n",
      "  HOMO Ridge    0.5506 0.0002\n",
      "  LUMO Ridge    0.8320 0.0004\n",
      "   gap Ridge    0.7829 0.0005\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRidge Regression Performance Metrics:\\n\")\n",
    "print(metrics_df.to_string(index=False, float_format=\"%.4f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d8977a",
   "metadata": {},
   "source": [
    "#### Train Random Forest Regression on each target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a0bea99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Regression Performance Metrics:\n",
      "\n",
      "target        model  r2_score    mse\n",
      "    mu RandomForest    0.6238 0.8338\n",
      "  HOMO RandomForest    0.8517 0.0001\n",
      "  LUMO RandomForest    0.9620 0.0001\n",
      "   gap RandomForest    0.9443 0.0001\n"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "for target in targets:\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    metrics.append({\n",
    "        \"target\": target,\n",
    "        \"model\": \"RandomForest\",\n",
    "        \"r2_score\": r2,\n",
    "        \"mse\": mse\n",
    "    })\n",
    "\n",
    "# Save to results/metrics\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df.to_csv(\"../results/metrics/random_forest_metrics.csv\", index=False)\n",
    "\n",
    "print(\"\\nRandom Forest Regression Performance Metrics:\\n\")\n",
    "print(metrics_df.to_string(index=False, float_format=\"%.4f\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ce623",
   "metadata": {},
   "source": [
    "#### Train Random Forest Regression on each target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85564955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Regression Performance Metrics:\n",
      "\n",
      "target   model  r2_score    mse\n",
      "    mu XGBoost    0.5884 0.9123\n",
      "  HOMO XGBoost    0.8363 0.0001\n",
      "  LUMO XGBoost    0.9525 0.0001\n",
      "   gap XGBoost    0.9283 0.0002\n"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "for target in targets:\n",
    "    y = df[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = XGBRegressor(random_state=42, n_jobs=-1, verbosity=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    metrics.append({\n",
    "        \"target\": target,\n",
    "        \"model\": \"XGBoost\",\n",
    "        \"r2_score\": r2,\n",
    "        \"mse\": mse\n",
    "    })\n",
    "\n",
    "# Save to results/metrics \n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df.to_csv(\"../results/metrics/xgboost_metrics.csv\", index=False)\n",
    "\n",
    "print(\"\\nXGBoost Regression Performance Metrics:\\n\")\n",
    "print(metrics_df.to_string(index=False, float_format=\"%.4f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcaccfb",
   "metadata": {},
   "source": [
    "#### Training NN regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "589bb0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN regression models\n",
    "from tensorflow.keras.regularizers import l2\n",
    "def train_nn_models(df,\n",
    "                    targets=None,\n",
    "                    epochs=100,\n",
    "                    batch_size=256,\n",
    "                    patience=7,\n",
    "                    l2_reg=1e-6,\n",
    "                    verbose=1,\n",
    "                    seed=42):\n",
    "    \"\"\"\n",
    "    Train separate NN regression models for each specified target.\n",
    "    Metrics are printed and returned as a DataFrame.\n",
    "    Optionally saves each model.\n",
    "    \"\"\"\n",
    "\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    #excluding identifiers and targets\n",
    "    excluded = [\"smiles\"] + targets\n",
    "    features = [col for col in df.columns if col not in excluded]\n",
    "\n",
    "    X = df[features].values\n",
    "    metrics = []\n",
    "\n",
    "    for target in targets:\n",
    "        y = df[target].values.reshape(-1, 1)\n",
    "\n",
    "        # Split\n",
    "        X_train, X_test, y_train_raw, y_test_raw = train_test_split(\n",
    "            X, y, test_size=0.18, random_state=seed\n",
    "        )\n",
    "\n",
    "        # Scale\n",
    "        X_scaler = StandardScaler()\n",
    "        y_scaler = StandardScaler()\n",
    "\n",
    "        X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "        y_train = y_scaler.fit_transform(y_train_raw)\n",
    "        y_test = y_scaler.transform(y_test_raw)\n",
    "\n",
    "        # Define model\n",
    "        model = Sequential([\n",
    "            Input(shape=(X_train_scaled.shape[1],)),\n",
    "            Dense(512, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "            Dropout(0.3),\n",
    "            Dense(256, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "            Dropout(0.3),\n",
    "            Dense(128, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "            Dropout(0.2),\n",
    "            Dense(64, activation='relu', kernel_regularizer=l2(l2_reg)),\n",
    "            Dense(1)\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "        # Early stopping\n",
    "        callbacks = [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=patience,\n",
    "                restore_best_weights=True,\n",
    "                verbose=verbose\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Train\n",
    "        history = model.fit(\n",
    "            X_train_scaled, y_train,\n",
    "            validation_data=(X_test_scaled, y_test),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "\n",
    "        # Predict\n",
    "        y_pred_scaled = model.predict(X_test_scaled, verbose=0)\n",
    "        y_pred = y_scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "        # Evaluate\n",
    "        r2 = r2_score(y_test_raw, y_pred)\n",
    "        mse = mean_squared_error(y_test_raw, y_pred)\n",
    "\n",
    "\n",
    "        print(f\"\\n{target.upper()} Evaluation:\")\n",
    "        print(f\"  MSE:       {mse:.5f}\")\n",
    "        print(f\"  R²:        {r2:.5f}\")\n",
    "\n",
    "        # Save metrics\n",
    "        metrics.append({\n",
    "            \"target\": target,\n",
    "            \"model\": \"TensorFlow_NN\",\n",
    "            \"MSE\": mse,\n",
    "            \"R2\": r2\n",
    "        })\n",
    "\n",
    "    # Print summary\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    print(\"\\nSummary of all models:\\n\")\n",
    "    print(metrics_df.to_string(index=False, float_format=\"%.5f\"))\n",
    "\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa61d8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.6715 - mae: 0.5992 - val_loss: 0.5114 - val_mae: 0.5265\n",
      "Epoch 2/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.5117 - mae: 0.5227 - val_loss: 0.4634 - val_mae: 0.5016\n",
      "Epoch 3/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.4827 - mae: 0.5042 - val_loss: 0.4471 - val_mae: 0.4918\n",
      "Epoch 4/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.4608 - mae: 0.4926 - val_loss: 0.4399 - val_mae: 0.4887\n",
      "Epoch 5/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.4567 - mae: 0.4856 - val_loss: 0.4346 - val_mae: 0.4874\n",
      "Epoch 6/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.4420 - mae: 0.4806 - val_loss: 0.4169 - val_mae: 0.4766\n",
      "Epoch 7/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.4297 - mae: 0.4749 - val_loss: 0.4068 - val_mae: 0.4644\n",
      "Epoch 8/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.4238 - mae: 0.4721 - val_loss: 0.4020 - val_mae: 0.4629\n",
      "Epoch 9/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.4185 - mae: 0.4679 - val_loss: 0.3973 - val_mae: 0.4627\n",
      "Epoch 10/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.4142 - mae: 0.4652 - val_loss: 0.3937 - val_mae: 0.4611\n",
      "Epoch 11/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.4103 - mae: 0.4620 - val_loss: 0.3893 - val_mae: 0.4572\n",
      "Epoch 12/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.4073 - mae: 0.4610 - val_loss: 0.3901 - val_mae: 0.4576\n",
      "Epoch 13/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.4025 - mae: 0.4580 - val_loss: 0.3849 - val_mae: 0.4557\n",
      "Epoch 14/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.3997 - mae: 0.4568 - val_loss: 0.3764 - val_mae: 0.4482\n",
      "Epoch 15/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3958 - mae: 0.4543 - val_loss: 0.3788 - val_mae: 0.4507\n",
      "Epoch 16/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3904 - mae: 0.4522 - val_loss: 0.3751 - val_mae: 0.4498\n",
      "Epoch 17/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3898 - mae: 0.4506 - val_loss: 0.3759 - val_mae: 0.4495\n",
      "Epoch 18/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.3870 - mae: 0.4485 - val_loss: 0.3759 - val_mae: 0.4503\n",
      "Epoch 19/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.3864 - mae: 0.4469 - val_loss: 0.3747 - val_mae: 0.4484\n",
      "Epoch 20/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3827 - mae: 0.4456 - val_loss: 0.3709 - val_mae: 0.4482\n",
      "Epoch 21/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3802 - mae: 0.4443 - val_loss: 0.3675 - val_mae: 0.4446\n",
      "Epoch 22/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3787 - mae: 0.4435 - val_loss: 0.3717 - val_mae: 0.4460\n",
      "Epoch 23/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3780 - mae: 0.4427 - val_loss: 0.3665 - val_mae: 0.4435\n",
      "Epoch 24/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3739 - mae: 0.4407 - val_loss: 0.3664 - val_mae: 0.4429\n",
      "Epoch 25/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3757 - mae: 0.4413 - val_loss: 0.3660 - val_mae: 0.4439\n",
      "Epoch 26/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3700 - mae: 0.4382 - val_loss: 0.3677 - val_mae: 0.4465\n",
      "Epoch 27/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.3639 - mae: 0.4361 - val_loss: 0.3626 - val_mae: 0.4417\n",
      "Epoch 28/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3684 - mae: 0.4370 - val_loss: 0.3598 - val_mae: 0.4403\n",
      "Epoch 29/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.3695 - mae: 0.4370 - val_loss: 0.3627 - val_mae: 0.4399\n",
      "Epoch 30/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.3693 - mae: 0.4380 - val_loss: 0.3596 - val_mae: 0.4430\n",
      "Epoch 31/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.3635 - mae: 0.4338 - val_loss: 0.3597 - val_mae: 0.4413\n",
      "Epoch 32/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.3604 - mae: 0.4329 - val_loss: 0.3533 - val_mae: 0.4349\n",
      "Epoch 33/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.3601 - mae: 0.4318 - val_loss: 0.3575 - val_mae: 0.4384\n",
      "Epoch 34/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.3603 - mae: 0.4321 - val_loss: 0.3580 - val_mae: 0.4380\n",
      "Epoch 35/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3588 - mae: 0.4310 - val_loss: 0.3526 - val_mae: 0.4345\n",
      "Epoch 36/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3572 - mae: 0.4306 - val_loss: 0.3585 - val_mae: 0.4407\n",
      "Epoch 37/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3549 - mae: 0.4293 - val_loss: 0.3537 - val_mae: 0.4369\n",
      "Epoch 38/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3515 - mae: 0.4278 - val_loss: 0.3620 - val_mae: 0.4432\n",
      "Epoch 39/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.3555 - mae: 0.4288 - val_loss: 0.3548 - val_mae: 0.4360\n",
      "Epoch 40/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.3532 - mae: 0.4278 - val_loss: 0.3520 - val_mae: 0.4355\n",
      "Epoch 41/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3465 - mae: 0.4252 - val_loss: 0.3518 - val_mae: 0.4307\n",
      "Epoch 42/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3503 - mae: 0.4257 - val_loss: 0.3507 - val_mae: 0.4341\n",
      "Epoch 43/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3507 - mae: 0.4261 - val_loss: 0.3505 - val_mae: 0.4329\n",
      "Epoch 44/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3486 - mae: 0.4242 - val_loss: 0.3495 - val_mae: 0.4335\n",
      "Epoch 45/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3456 - mae: 0.4222 - val_loss: 0.3469 - val_mae: 0.4305\n",
      "Epoch 46/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3466 - mae: 0.4235 - val_loss: 0.3493 - val_mae: 0.4307\n",
      "Epoch 47/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3466 - mae: 0.4244 - val_loss: 0.3463 - val_mae: 0.4289\n",
      "Epoch 48/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3427 - mae: 0.4217 - val_loss: 0.3508 - val_mae: 0.4350\n",
      "Epoch 49/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3478 - mae: 0.4220 - val_loss: 0.3478 - val_mae: 0.4319\n",
      "Epoch 50/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3434 - mae: 0.4209 - val_loss: 0.3446 - val_mae: 0.4275\n",
      "Epoch 51/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3414 - mae: 0.4202 - val_loss: 0.3498 - val_mae: 0.4335\n",
      "Epoch 52/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3403 - mae: 0.4194 - val_loss: 0.3457 - val_mae: 0.4277\n",
      "Epoch 53/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3413 - mae: 0.4202 - val_loss: 0.3479 - val_mae: 0.4317\n",
      "Epoch 54/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3378 - mae: 0.4186 - val_loss: 0.3460 - val_mae: 0.4313\n",
      "Epoch 55/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3408 - mae: 0.4196 - val_loss: 0.3486 - val_mae: 0.4318\n",
      "Epoch 56/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3381 - mae: 0.4178 - val_loss: 0.3516 - val_mae: 0.4346\n",
      "Epoch 57/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3378 - mae: 0.4178 - val_loss: 0.3419 - val_mae: 0.4269\n",
      "Epoch 58/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3368 - mae: 0.4164 - val_loss: 0.3474 - val_mae: 0.4329\n",
      "Epoch 59/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3381 - mae: 0.4174 - val_loss: 0.3440 - val_mae: 0.4285\n",
      "Epoch 60/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3374 - mae: 0.4165 - val_loss: 0.3424 - val_mae: 0.4267\n",
      "Epoch 61/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3392 - mae: 0.4171 - val_loss: 0.3415 - val_mae: 0.4261\n",
      "Epoch 62/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3349 - mae: 0.4148 - val_loss: 0.3441 - val_mae: 0.4287\n",
      "Epoch 63/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3390 - mae: 0.4173 - val_loss: 0.3418 - val_mae: 0.4259\n",
      "Epoch 64/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3323 - mae: 0.4140 - val_loss: 0.3463 - val_mae: 0.4306\n",
      "Epoch 65/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3312 - mae: 0.4139 - val_loss: 0.3452 - val_mae: 0.4297\n",
      "Epoch 66/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3305 - mae: 0.4137 - val_loss: 0.3426 - val_mae: 0.4275\n",
      "Epoch 67/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3323 - mae: 0.4141 - val_loss: 0.3441 - val_mae: 0.4291\n",
      "Epoch 68/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.3290 - mae: 0.4124 - val_loss: 0.3457 - val_mae: 0.4289\n",
      "Epoch 68: early stopping\n",
      "Restoring model weights from the end of the best epoch: 61.\n",
      "\n",
      "MU Evaluation:\n",
      "  MSE:       0.76793\n",
      "  R²:        0.65303\n",
      "Epoch 1/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.2538 - mae: 0.3857 - val_loss: 0.1533 - val_mae: 0.3038\n",
      "Epoch 2/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.1272 - mae: 0.2738 - val_loss: 0.1395 - val_mae: 0.2874\n",
      "Epoch 3/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.1096 - mae: 0.2527 - val_loss: 0.1139 - val_mae: 0.2572\n",
      "Epoch 4/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.1001 - mae: 0.2403 - val_loss: 0.1244 - val_mae: 0.2692\n",
      "Epoch 5/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0925 - mae: 0.2303 - val_loss: 0.1012 - val_mae: 0.2424\n",
      "Epoch 6/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0880 - mae: 0.2247 - val_loss: 0.0992 - val_mae: 0.2398\n",
      "Epoch 7/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0852 - mae: 0.2207 - val_loss: 0.0980 - val_mae: 0.2387\n",
      "Epoch 8/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0820 - mae: 0.2166 - val_loss: 0.0925 - val_mae: 0.2316\n",
      "Epoch 9/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0800 - mae: 0.2131 - val_loss: 0.0922 - val_mae: 0.2302\n",
      "Epoch 10/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0780 - mae: 0.2102 - val_loss: 0.0887 - val_mae: 0.2252\n",
      "Epoch 11/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0750 - mae: 0.2063 - val_loss: 0.0859 - val_mae: 0.2216\n",
      "Epoch 12/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0737 - mae: 0.2041 - val_loss: 0.0809 - val_mae: 0.2143\n",
      "Epoch 13/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0722 - mae: 0.2020 - val_loss: 0.0821 - val_mae: 0.2164\n",
      "Epoch 14/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0720 - mae: 0.2013 - val_loss: 0.0810 - val_mae: 0.2143\n",
      "Epoch 15/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0703 - mae: 0.1990 - val_loss: 0.0766 - val_mae: 0.2082\n",
      "Epoch 16/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0686 - mae: 0.1963 - val_loss: 0.0696 - val_mae: 0.1983\n",
      "Epoch 17/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0683 - mae: 0.1955 - val_loss: 0.0725 - val_mae: 0.2023\n",
      "Epoch 18/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0671 - mae: 0.1937 - val_loss: 0.0684 - val_mae: 0.1963\n",
      "Epoch 19/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0668 - mae: 0.1931 - val_loss: 0.0685 - val_mae: 0.1957\n",
      "Epoch 20/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0655 - mae: 0.1912 - val_loss: 0.0643 - val_mae: 0.1892\n",
      "Epoch 21/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0664 - mae: 0.1916 - val_loss: 0.0655 - val_mae: 0.1917\n",
      "Epoch 22/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0644 - mae: 0.1888 - val_loss: 0.0624 - val_mae: 0.1863\n",
      "Epoch 23/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0634 - mae: 0.1876 - val_loss: 0.0624 - val_mae: 0.1863\n",
      "Epoch 24/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0640 - mae: 0.1881 - val_loss: 0.0618 - val_mae: 0.1843\n",
      "Epoch 25/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0625 - mae: 0.1858 - val_loss: 0.0616 - val_mae: 0.1837\n",
      "Epoch 26/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0628 - mae: 0.1865 - val_loss: 0.0625 - val_mae: 0.1860\n",
      "Epoch 27/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0615 - mae: 0.1842 - val_loss: 0.0600 - val_mae: 0.1819\n",
      "Epoch 28/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0616 - mae: 0.1839 - val_loss: 0.0585 - val_mae: 0.1779\n",
      "Epoch 29/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0609 - mae: 0.1827 - val_loss: 0.0586 - val_mae: 0.1785\n",
      "Epoch 30/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0599 - mae: 0.1816 - val_loss: 0.0551 - val_mae: 0.1732\n",
      "Epoch 31/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0604 - mae: 0.1820 - val_loss: 0.0582 - val_mae: 0.1767\n",
      "Epoch 32/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0595 - mae: 0.1808 - val_loss: 0.0576 - val_mae: 0.1763\n",
      "Epoch 33/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0598 - mae: 0.1807 - val_loss: 0.0560 - val_mae: 0.1737\n",
      "Epoch 34/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0596 - mae: 0.1803 - val_loss: 0.0561 - val_mae: 0.1747\n",
      "Epoch 35/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0584 - mae: 0.1786 - val_loss: 0.0548 - val_mae: 0.1716\n",
      "Epoch 36/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0586 - mae: 0.1785 - val_loss: 0.0567 - val_mae: 0.1744\n",
      "Epoch 37/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0576 - mae: 0.1774 - val_loss: 0.0546 - val_mae: 0.1702\n",
      "Epoch 38/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0577 - mae: 0.1775 - val_loss: 0.0542 - val_mae: 0.1704\n",
      "Epoch 39/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0579 - mae: 0.1769 - val_loss: 0.0538 - val_mae: 0.1685\n",
      "Epoch 40/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0581 - mae: 0.1771 - val_loss: 0.0576 - val_mae: 0.1752\n",
      "Epoch 41/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0574 - mae: 0.1760 - val_loss: 0.0513 - val_mae: 0.1657\n",
      "Epoch 42/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0567 - mae: 0.1751 - val_loss: 0.0556 - val_mae: 0.1722\n",
      "Epoch 43/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0570 - mae: 0.1754 - val_loss: 0.0535 - val_mae: 0.1683\n",
      "Epoch 44/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0563 - mae: 0.1743 - val_loss: 0.0535 - val_mae: 0.1688\n",
      "Epoch 45/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0563 - mae: 0.1741 - val_loss: 0.0542 - val_mae: 0.1702\n",
      "Epoch 46/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0561 - mae: 0.1738 - val_loss: 0.0531 - val_mae: 0.1681\n",
      "Epoch 47/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0555 - mae: 0.1727 - val_loss: 0.0529 - val_mae: 0.1672\n",
      "Epoch 48/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0559 - mae: 0.1733 - val_loss: 0.0552 - val_mae: 0.1709\n",
      "Epoch 48: early stopping\n",
      "Restoring model weights from the end of the best epoch: 41.\n",
      "\n",
      "GAP Evaluation:\n",
      "  MSE:       0.00011\n",
      "  R²:        0.95195\n",
      "Epoch 1/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.2434 - mae: 0.3689 - val_loss: 0.1240 - val_mae: 0.2822\n",
      "Epoch 2/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0933 - mae: 0.2333 - val_loss: 0.0932 - val_mae: 0.2452\n",
      "Epoch 3/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0776 - mae: 0.2109 - val_loss: 0.0813 - val_mae: 0.2255\n",
      "Epoch 4/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0687 - mae: 0.1980 - val_loss: 0.0712 - val_mae: 0.2094\n",
      "Epoch 5/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0633 - mae: 0.1893 - val_loss: 0.0646 - val_mae: 0.1981\n",
      "Epoch 6/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0593 - mae: 0.1828 - val_loss: 0.0601 - val_mae: 0.1903\n",
      "Epoch 7/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0565 - mae: 0.1781 - val_loss: 0.0577 - val_mae: 0.1864\n",
      "Epoch 8/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0538 - mae: 0.1736 - val_loss: 0.0518 - val_mae: 0.1747\n",
      "Epoch 9/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0523 - mae: 0.1708 - val_loss: 0.0517 - val_mae: 0.1741\n",
      "Epoch 10/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0513 - mae: 0.1687 - val_loss: 0.0468 - val_mae: 0.1644\n",
      "Epoch 11/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0496 - mae: 0.1651 - val_loss: 0.0462 - val_mae: 0.1619\n",
      "Epoch 12/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0487 - mae: 0.1636 - val_loss: 0.0453 - val_mae: 0.1611\n",
      "Epoch 13/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0479 - mae: 0.1623 - val_loss: 0.0426 - val_mae: 0.1548\n",
      "Epoch 14/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0472 - mae: 0.1609 - val_loss: 0.0421 - val_mae: 0.1547\n",
      "Epoch 15/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0462 - mae: 0.1593 - val_loss: 0.0440 - val_mae: 0.1580\n",
      "Epoch 16/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0457 - mae: 0.1580 - val_loss: 0.0419 - val_mae: 0.1525\n",
      "Epoch 17/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0452 - mae: 0.1574 - val_loss: 0.0415 - val_mae: 0.1500\n",
      "Epoch 18/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0447 - mae: 0.1562 - val_loss: 0.0399 - val_mae: 0.1489\n",
      "Epoch 19/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0443 - mae: 0.1552 - val_loss: 0.0432 - val_mae: 0.1546\n",
      "Epoch 20/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0436 - mae: 0.1537 - val_loss: 0.0435 - val_mae: 0.1527\n",
      "Epoch 21/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.0427 - mae: 0.1520 - val_loss: 0.0401 - val_mae: 0.1486\n",
      "Epoch 22/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0426 - mae: 0.1518 - val_loss: 0.0404 - val_mae: 0.1461\n",
      "Epoch 23/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0423 - mae: 0.1512 - val_loss: 0.0383 - val_mae: 0.1444\n",
      "Epoch 24/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0416 - mae: 0.1495 - val_loss: 0.0393 - val_mae: 0.1445\n",
      "Epoch 25/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0414 - mae: 0.1490 - val_loss: 0.0402 - val_mae: 0.1459\n",
      "Epoch 26/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0408 - mae: 0.1478 - val_loss: 0.0396 - val_mae: 0.1439\n",
      "Epoch 27/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0410 - mae: 0.1485 - val_loss: 0.0382 - val_mae: 0.1423\n",
      "Epoch 28/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0410 - mae: 0.1481 - val_loss: 0.0373 - val_mae: 0.1407\n",
      "Epoch 29/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0402 - mae: 0.1467 - val_loss: 0.0406 - val_mae: 0.1452\n",
      "Epoch 30/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0402 - mae: 0.1466 - val_loss: 0.0386 - val_mae: 0.1415\n",
      "Epoch 31/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0399 - mae: 0.1459 - val_loss: 0.0383 - val_mae: 0.1439\n",
      "Epoch 32/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0395 - mae: 0.1450 - val_loss: 0.0392 - val_mae: 0.1433\n",
      "Epoch 33/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0393 - mae: 0.1445 - val_loss: 0.0368 - val_mae: 0.1396\n",
      "Epoch 34/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0392 - mae: 0.1441 - val_loss: 0.0365 - val_mae: 0.1378\n",
      "Epoch 35/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0387 - mae: 0.1435 - val_loss: 0.0397 - val_mae: 0.1419\n",
      "Epoch 36/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0391 - mae: 0.1440 - val_loss: 0.0365 - val_mae: 0.1376\n",
      "Epoch 37/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0388 - mae: 0.1434 - val_loss: 0.0361 - val_mae: 0.1373\n",
      "Epoch 38/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0381 - mae: 0.1416 - val_loss: 0.0348 - val_mae: 0.1359\n",
      "Epoch 39/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0380 - mae: 0.1418 - val_loss: 0.0371 - val_mae: 0.1398\n",
      "Epoch 40/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0382 - mae: 0.1420 - val_loss: 0.0367 - val_mae: 0.1386\n",
      "Epoch 41/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0380 - mae: 0.1415 - val_loss: 0.0394 - val_mae: 0.1421\n",
      "Epoch 42/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0381 - mae: 0.1414 - val_loss: 0.0378 - val_mae: 0.1380\n",
      "Epoch 43/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0378 - mae: 0.1408 - val_loss: 0.0377 - val_mae: 0.1379\n",
      "Epoch 44/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0371 - mae: 0.1401 - val_loss: 0.0373 - val_mae: 0.1377\n",
      "Epoch 45/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.0381 - mae: 0.1411 - val_loss: 0.0374 - val_mae: 0.1395\n",
      "Epoch 45: early stopping\n",
      "Restoring model weights from the end of the best epoch: 38.\n",
      "\n",
      "LUMO Evaluation:\n",
      "  MSE:       0.00007\n",
      "  R²:        0.96739\n",
      "Epoch 1/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.4517 - mae: 0.4976 - val_loss: 0.2238 - val_mae: 0.3653\n",
      "Epoch 2/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.2538 - mae: 0.3913 - val_loss: 0.1976 - val_mae: 0.3450\n",
      "Epoch 3/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.2214 - mae: 0.3653 - val_loss: 0.1830 - val_mae: 0.3302\n",
      "Epoch 4/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.2061 - mae: 0.3517 - val_loss: 0.1824 - val_mae: 0.3300\n",
      "Epoch 5/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1947 - mae: 0.3421 - val_loss: 0.1679 - val_mae: 0.3161\n",
      "Epoch 6/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1876 - mae: 0.3350 - val_loss: 0.1645 - val_mae: 0.3106\n",
      "Epoch 7/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1804 - mae: 0.3284 - val_loss: 0.1586 - val_mae: 0.3065\n",
      "Epoch 8/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1739 - mae: 0.3215 - val_loss: 0.1506 - val_mae: 0.2981\n",
      "Epoch 9/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1706 - mae: 0.3180 - val_loss: 0.1557 - val_mae: 0.3014\n",
      "Epoch 10/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1649 - mae: 0.3133 - val_loss: 0.1471 - val_mae: 0.2942\n",
      "Epoch 11/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1614 - mae: 0.3092 - val_loss: 0.1494 - val_mae: 0.2940\n",
      "Epoch 12/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1580 - mae: 0.3056 - val_loss: 0.1409 - val_mae: 0.2870\n",
      "Epoch 13/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1547 - mae: 0.3022 - val_loss: 0.1469 - val_mae: 0.2914\n",
      "Epoch 14/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1536 - mae: 0.3008 - val_loss: 0.1414 - val_mae: 0.2863\n",
      "Epoch 15/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1514 - mae: 0.2983 - val_loss: 0.1453 - val_mae: 0.2887\n",
      "Epoch 16/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1479 - mae: 0.2946 - val_loss: 0.1409 - val_mae: 0.2849\n",
      "Epoch 17/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1471 - mae: 0.2934 - val_loss: 0.1363 - val_mae: 0.2797\n",
      "Epoch 18/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1444 - mae: 0.2901 - val_loss: 0.1321 - val_mae: 0.2754\n",
      "Epoch 19/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1437 - mae: 0.2900 - val_loss: 0.1405 - val_mae: 0.2843\n",
      "Epoch 20/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1412 - mae: 0.2873 - val_loss: 0.1313 - val_mae: 0.2756\n",
      "Epoch 21/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1399 - mae: 0.2859 - val_loss: 0.1377 - val_mae: 0.2812\n",
      "Epoch 22/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1385 - mae: 0.2840 - val_loss: 0.1275 - val_mae: 0.2713\n",
      "Epoch 23/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1372 - mae: 0.2831 - val_loss: 0.1289 - val_mae: 0.2725\n",
      "Epoch 24/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1360 - mae: 0.2812 - val_loss: 0.1266 - val_mae: 0.2700\n",
      "Epoch 25/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1346 - mae: 0.2795 - val_loss: 0.1314 - val_mae: 0.2749\n",
      "Epoch 26/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1342 - mae: 0.2787 - val_loss: 0.1254 - val_mae: 0.2675\n",
      "Epoch 27/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1348 - mae: 0.2793 - val_loss: 0.1289 - val_mae: 0.2716\n",
      "Epoch 28/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1332 - mae: 0.2776 - val_loss: 0.1210 - val_mae: 0.2629\n",
      "Epoch 29/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1320 - mae: 0.2759 - val_loss: 0.1245 - val_mae: 0.2666\n",
      "Epoch 30/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1307 - mae: 0.2745 - val_loss: 0.1226 - val_mae: 0.2647\n",
      "Epoch 31/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1288 - mae: 0.2734 - val_loss: 0.1313 - val_mae: 0.2737\n",
      "Epoch 32/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1286 - mae: 0.2725 - val_loss: 0.1240 - val_mae: 0.2661\n",
      "Epoch 33/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1273 - mae: 0.2716 - val_loss: 0.1288 - val_mae: 0.2708\n",
      "Epoch 34/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1276 - mae: 0.2713 - val_loss: 0.1194 - val_mae: 0.2604\n",
      "Epoch 35/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1269 - mae: 0.2702 - val_loss: 0.1242 - val_mae: 0.2648\n",
      "Epoch 36/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1254 - mae: 0.2693 - val_loss: 0.1244 - val_mae: 0.2656\n",
      "Epoch 37/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1257 - mae: 0.2689 - val_loss: 0.1280 - val_mae: 0.2697\n",
      "Epoch 38/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1249 - mae: 0.2678 - val_loss: 0.1238 - val_mae: 0.2651\n",
      "Epoch 39/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1244 - mae: 0.2675 - val_loss: 0.1218 - val_mae: 0.2629\n",
      "Epoch 40/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1234 - mae: 0.2667 - val_loss: 0.1254 - val_mae: 0.2651\n",
      "Epoch 41/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1231 - mae: 0.2657 - val_loss: 0.1178 - val_mae: 0.2579\n",
      "Epoch 42/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.1228 - mae: 0.2650 - val_loss: 0.1284 - val_mae: 0.2696\n",
      "Epoch 43/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1221 - mae: 0.2646 - val_loss: 0.1262 - val_mae: 0.2672\n",
      "Epoch 44/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1206 - mae: 0.2629 - val_loss: 0.1201 - val_mae: 0.2607\n",
      "Epoch 45/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1218 - mae: 0.2643 - val_loss: 0.1240 - val_mae: 0.2643\n",
      "Epoch 46/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1199 - mae: 0.2616 - val_loss: 0.1200 - val_mae: 0.2602\n",
      "Epoch 47/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1200 - mae: 0.2619 - val_loss: 0.1178 - val_mae: 0.2579\n",
      "Epoch 48/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1203 - mae: 0.2621 - val_loss: 0.1213 - val_mae: 0.2610\n",
      "Epoch 49/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1193 - mae: 0.2609 - val_loss: 0.1214 - val_mae: 0.2614\n",
      "Epoch 50/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1189 - mae: 0.2603 - val_loss: 0.1178 - val_mae: 0.2573\n",
      "Epoch 51/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1187 - mae: 0.2596 - val_loss: 0.1243 - val_mae: 0.2640\n",
      "Epoch 52/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1187 - mae: 0.2604 - val_loss: 0.1176 - val_mae: 0.2570\n",
      "Epoch 53/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1182 - mae: 0.2590 - val_loss: 0.1171 - val_mae: 0.2559\n",
      "Epoch 54/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1162 - mae: 0.2577 - val_loss: 0.1208 - val_mae: 0.2613\n",
      "Epoch 55/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1174 - mae: 0.2588 - val_loss: 0.1171 - val_mae: 0.2558\n",
      "Epoch 56/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1162 - mae: 0.2572 - val_loss: 0.1148 - val_mae: 0.2535\n",
      "Epoch 57/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1158 - mae: 0.2561 - val_loss: 0.1181 - val_mae: 0.2576\n",
      "Epoch 58/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1167 - mae: 0.2573 - val_loss: 0.1161 - val_mae: 0.2552\n",
      "Epoch 59/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1151 - mae: 0.2558 - val_loss: 0.1198 - val_mae: 0.2589\n",
      "Epoch 60/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1152 - mae: 0.2559 - val_loss: 0.1268 - val_mae: 0.2667\n",
      "Epoch 61/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1156 - mae: 0.2560 - val_loss: 0.1124 - val_mae: 0.2499\n",
      "Epoch 62/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1146 - mae: 0.2550 - val_loss: 0.1226 - val_mae: 0.2619\n",
      "Epoch 63/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1141 - mae: 0.2545 - val_loss: 0.1255 - val_mae: 0.2655\n",
      "Epoch 64/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1141 - mae: 0.2541 - val_loss: 0.1224 - val_mae: 0.2616\n",
      "Epoch 65/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1142 - mae: 0.2540 - val_loss: 0.1202 - val_mae: 0.2588\n",
      "Epoch 66/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1138 - mae: 0.2537 - val_loss: 0.1196 - val_mae: 0.2573\n",
      "Epoch 67/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1147 - mae: 0.2537 - val_loss: 0.1197 - val_mae: 0.2577\n",
      "Epoch 68/100\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1126 - mae: 0.2524 - val_loss: 0.1153 - val_mae: 0.2531\n",
      "Epoch 68: early stopping\n",
      "Restoring model weights from the end of the best epoch: 61.\n",
      "\n",
      "HOMO Evaluation:\n",
      "  MSE:       0.00005\n",
      "  R²:        0.89003\n",
      "\n",
      "Summary of all models:\n",
      "\n",
      "target         model     MSE      R2\n",
      "    mu TensorFlow_NN 0.76793 0.65303\n",
      "   gap TensorFlow_NN 0.00011 0.95195\n",
      "  LUMO TensorFlow_NN 0.00007 0.96739\n",
      "  HOMO TensorFlow_NN 0.00005 0.89003\n"
     ]
    }
   ],
   "source": [
    "metrics_df = train_nn_models(df, targets=[\"mu\", \"gap\",\"LUMO\", \"HOMO\"])\n",
    "metrics_df.to_csv(\"../results/metrics/tf_nn_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fcc25b",
   "metadata": {},
   "source": [
    "#### Train SVM for each target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "631d1fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MU SVM Evaluation:\n",
      "  MSE:       1.03210\n",
      "  R²:        0.53437\n",
      "\n",
      "HOMO SVM Evaluation:\n",
      "  MSE:       0.00008\n",
      "  R²:        0.84151\n",
      "\n",
      "LUMO SVM Evaluation:\n",
      "  MSE:       0.00010\n",
      "  R²:        0.95512\n",
      "\n",
      "GAP SVM Evaluation:\n",
      "  MSE:       0.00016\n",
      "  R²:        0.92630\n",
      "\n",
      "SVM Regression Summary:\n",
      "\n",
      "target model     MSE      R2\n",
      "    mu   SVM 1.03210 0.53437\n",
      "  HOMO   SVM 0.00008 0.84151\n",
      "  LUMO   SVM 0.00010 0.95512\n",
      "   gap   SVM 0.00016 0.92630\n"
     ]
    }
   ],
   "source": [
    "# Feature scaling\n",
    "X_scaler = StandardScaler()\n",
    "X_scaled = X_scaler.fit_transform(X)\n",
    "\n",
    "metrics = []\n",
    "for target in targets:\n",
    "    y = df[target].values.reshape(-1, 1)\n",
    "    y_scaler = StandardScaler()\n",
    "    y_scaled = y_scaler.fit_transform(y).ravel()  # flatten for SVR\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_scaled, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_scaled = model.predict(X_test)\n",
    "\n",
    "    # Inverse scale predictions\n",
    "    y_test_unscaled = y_scaler.inverse_transform(y_test.reshape(-1, 1)).ravel()\n",
    "    y_pred_unscaled = y_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()\n",
    "\n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y_test_unscaled, y_pred_unscaled)\n",
    "    r2 = r2_score(y_test_unscaled, y_pred_unscaled)\n",
    "\n",
    "    print(f\"\\n{target.upper()} SVM Evaluation:\")\n",
    "    print(f\"  MSE:       {mse:.5f}\")\n",
    "    print(f\"  R²:        {r2:.5f}\")\n",
    "\n",
    "    metrics.append({\n",
    "        \"target\": target,\n",
    "        \"model\": \"SVM\",\n",
    "        \"MSE\": mse,\n",
    "        \"R2\": r2\n",
    "    })\n",
    "\n",
    "# Save Metrics \n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df.to_csv(\"../results/metrics/svm_metrics.csv\", index=False)\n",
    "\n",
    "print(\"\\nSVM Regression Summary:\\n\")\n",
    "print(metrics_df.to_string(index=False, float_format=\"%.5f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b414b1",
   "metadata": {},
   "source": [
    "#### Stacked Ensemble (Ridge + RF + XGB + SVM + CatBoost + LGBM + GB → LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4362a7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkasiror\\AppData\\Local\\miniconda3\\envs\\pyata-book\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "MU - Stacked Model Evaluation:\n",
      "  MSE:       0.81134\n",
      "  R²:        0.63397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkasiror\\AppData\\Local\\miniconda3\\envs\\pyata-book\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "HOMO - Stacked Model Evaluation:\n",
      "  MSE:       0.00006\n",
      "  R²:        0.87317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkasiror\\AppData\\Local\\miniconda3\\envs\\pyata-book\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LUMO - Stacked Model Evaluation:\n",
      "  MSE:       0.00008\n",
      "  R²:        0.96566\n",
      "\n",
      "GAP - Stacked Model Evaluation:\n",
      "  MSE:       0.00011\n",
      "  R²:        0.94874\n",
      "\n",
      "Stacked Model Summary:\n",
      "\n",
      "target            model     MSE      R2\n",
      "    mu StackedRegressor 0.81134 0.63397\n",
      "  HOMO StackedRegressor 0.00006 0.87317\n",
      "  LUMO StackedRegressor 0.00008 0.96566\n",
      "   gap StackedRegressor 0.00011 0.94874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkasiror\\AppData\\Local\\miniconda3\\envs\\pyata-book\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "metrics = []\n",
    "# Train for each target \n",
    "for target in targets:\n",
    "    y = df[target].values\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    #Base learners\n",
    "    base_learners = [\n",
    "        (\"ridge\", Ridge(alpha=1.0)),\n",
    "        (\"rf\", RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)),\n",
    "        (\"xgb\", XGBRegressor(n_estimators=100, random_state=42, verbosity=0, n_jobs=-1)),\n",
    "        (\"svm\", SVR(kernel=\"rbf\", C=1.0, epsilon=0.1)),\n",
    "        (\"gb\", GradientBoostingRegressor(n_estimators=100, random_state=42)),\n",
    "        (\"cat\", CatBoostRegressor(verbose=0, random_seed=42)),\n",
    "        (\"lgbm\", LGBMRegressor(n_estimators=100, random_state=42))\n",
    "    ]\n",
    "\n",
    "    # Meta-learner\n",
    "    meta_model = LinearRegression()\n",
    "\n",
    "    # Stacking model\n",
    "    stack = StackingRegressor(\n",
    "        estimators=base_learners,\n",
    "        final_estimator=meta_model,\n",
    "        passthrough=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Pipeline\n",
    "    pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"stacked\", stack)\n",
    "    ])\n",
    "\n",
    "    # Train\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"\\n{target.upper()} - Stacked Model Evaluation:\")\n",
    "    print(f\"  MSE:       {mse:.5f}\")\n",
    "    print(f\"  R²:        {r2:.5f}\")\n",
    "\n",
    "    # Save metrics\n",
    "    metrics.append({\n",
    "        \"target\": target,\n",
    "        \"model\": \"StackedRegressor\",\n",
    "        \"MSE\": mse,\n",
    "        \"R2\": r2\n",
    "    })\n",
    "\n",
    "# Save metrics to CSV\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df.to_csv(\"../results/metrics/stacked_model_metrics.csv\", index=False)\n",
    "\n",
    "print(\"\\nStacked Model Summary:\\n\")\n",
    "print(metrics_df.to_string(index=False, float_format=\"%.5f\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21a58dc",
   "metadata": {},
   "source": [
    "### Using a stacked approach that first learns HOMO, LUMO, and gap and then uses these predictions to predict mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ecd092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 1: Predict HOMO and LUMO from descriptors\n",
    "def train_and_predict_qm_props(df, targets=['HOMO', 'LUMO'], seed=42, results_path=\"../results/metrics\"):\n",
    "    \"\"\"\n",
    "    Train Random Forest models to predict HOMO and LUMO from descriptors.\n",
    "    Saves predictions for train/test and logs metrics for each target.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Define input features\n",
    "    features = [col for col in df.columns if col not in ['smiles'] + targets + ['mu', 'gap']]\n",
    "    X = df[features].values\n",
    "\n",
    "    # Split features and full DF (for retaining original labels)\n",
    "    X_train, X_test, df_train, df_test = train_test_split(\n",
    "        X, df.copy(), test_size=0.18, random_state=seed\n",
    "    )\n",
    "\n",
    "    # Scale features\n",
    "    X_scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = X_scaler.transform(X_train)\n",
    "    X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "    # DataFrames to hold predictions\n",
    "    pred_train_df = df_train.copy()\n",
    "    pred_test_df = df_test.copy()\n",
    "\n",
    "    # Track metrics\n",
    "    metrics = []\n",
    "\n",
    "    for target in targets:\n",
    "        y_train = df_train[target].values\n",
    "        y_test = df_test[target].values\n",
    "\n",
    "        model = RandomForestRegressor(random_state=seed, n_jobs=-1)\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Predictions\n",
    "        y_pred_train = model.predict(X_train_scaled)\n",
    "        y_pred_test = model.predict(X_test_scaled)\n",
    "\n",
    "        pred_train_df[f'pred_{target}'] = y_pred_train\n",
    "        pred_test_df[f'pred_{target}'] = y_pred_test\n",
    "\n",
    "        # Evaluate\n",
    "        r2 = r2_score(y_test, y_pred_test)\n",
    "        mse = mean_squared_error(y_test, y_pred_test)\n",
    "        metrics.append({\n",
    "            \"target\": target,\n",
    "            \"model\": \"RandomForest\",\n",
    "            \"r2_score\": r2,\n",
    "            \"mse\": mse\n",
    "        })\n",
    "\n",
    "    # Save metrics\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    print(\"\\nRandom Forest Regression Performance Metrics:\\n\")\n",
    "    print(metrics_df.to_string(index=False, float_format=\"%.4f\"))\n",
    "\n",
    "    return pred_train_df, pred_test_df, X_scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "16e41397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Regression Performance Metrics:\n",
      "\n",
      "target        model  r2_score    mse\n",
      "  HOMO RandomForest    0.8529 0.0001\n",
      "  LUMO RandomForest    0.9626 0.0001\n",
      "   gap RandomForest    0.9452 0.0001\n"
     ]
    }
   ],
   "source": [
    "pred_train_df, pred_test_df, _ = train_and_predict_qm_props(df, targets=['HOMO', 'LUMO', 'gap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0b6ecf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 2: Predict dipole moment using predictions from Stage 1\n",
    "def train_dipole_with_qm_preds(pred_train_df, pred_test_df, seed=42):\n",
    "    \"\"\"\n",
    "    Train model to predict dipole moment using original features + predicted HOMO, LUMO\n",
    "    \"\"\"\n",
    "    # Targets and Features\n",
    "    used_features = [col for col in pred_train_df.columns if col.startswith('pred_') or col not in ['smiles', 'mu', 'HOMO', 'LUMO', 'gap']]\n",
    "    \n",
    "    X_train = pred_train_df[used_features].values\n",
    "    y_train = pred_train_df['mu'].values\n",
    "\n",
    "    X_test = pred_test_df[used_features].values\n",
    "    y_test = pred_test_df['mu'].values\n",
    "\n",
    "    # Optional: Normalize (could experiment with raw vs. scaled)\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    model = GradientBoostingRegressor(random_state=seed)\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    print(\"\\nDIPOLE MOMENT PREDICTION:\")\n",
    "    print(f\"  MSE: {mean_squared_error(y_test, y_pred):.5f}\")\n",
    "    print(f\"  R²:  {r2_score(y_test, y_pred):.5f}\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6dcd585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DIPOLE MOMENT PREDICTION:\n",
      "  MSE: 1.16975\n",
      "  R²:  0.47147\n"
     ]
    }
   ],
   "source": [
    "mu_model = train_dipole_with_qm_preds(pred_train_df, pred_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8172ead1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyata-book",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
